# 思路和文件说明

## Model 1（LSTM）这个属于瞎搭建，刚开始失误很多，纯当打草稿了。可以跳过这一部分。

我的目标是为每个交易日提取一个代表性的特征向量，首先想到的是，这个向量既要包含当日的信息，最好能记录一下“时序”性质，我就先采用LSTM试了一下。我的代码几乎都是prompt写的，况且我还不够了解LSTM，所以我也不清楚模型具体为什么要这么构建。待我补了一些gap之后，我会再来核查这种做法的合理性。

### 模型结构

#### 1. LSTM 层 (`self.lstm`)

- **输入**：这一层接受的输入是一个形状为 `[batch_size, sequence_length, input_size]` 的张量。在我的案例中，`batch_size` 代表交易日数，`sequence_length` 对应股票数量，而 `input_size` 是每只股票的特征数。
- **输出**：LSTM 的输出是一个形状为 `[batch_size, sequence_length, hidden_size]` 的张量。其中 `hidden_size` 是 LSTM 单元的隐藏状态大小。

#### 2. 线性输出层 (`self.out`)

- **输入**：这一层的输入是 LSTM 层的输出，其形状为 `[batch_size, sequence_length, hidden_size]`。
- **输出**：该层的输出是一个形状为 `[batch_size, sequence_length, output_size]` 的张量。`output_size` 设置为1，因为我希望为每只股票预测一个标签值。

### 提取特征向量

在 `LSTMNet` 中，我用的 LSTM 层的隐藏状态作为反映每个交易日特点的特征向量。具体已经保存在了`features_dict_epoch_10.pkl`里面，直接读取即可。

### 合理性？

这样做还是有一定道理，毕竟隐藏状态提供了数据的压缩表示，不仅反映了当前交易日的特征，还包含之前的信息。但是首先是维度问题，这里的特征向量长度为50，不知道是大是小；再就是这是一个黑盒表示，很难直接解释。

我感觉我这个模型是做失败了的，我还试过线性的网络，搞过一天，失败了，我决定用RNN试试。

## Model 2（RNN）

拟训练一个RNN模型，把每个`time_id`通过训练好的模型映射到隐藏状态作为特征向量。

这次用了RNN，有仍有瞎搭建之嫌，不过在model1的基础上长了教训，我首先干脆直接去掉了有缺失值的数据，并且在模型的最后一层采用了动态调整全连接层维度的方式（我在网上搜到过这种类似于混合神经网络的trick，这一层是必要的不然输出维度不匹配，但是我做的优化太少，几乎没有，就一层暴力全连接层，甚至没有写额外的代码，这是我比较担心的一个方面），下面是ChatGPT生成的这个网络的各层信息：

#### **StockRNN**

- **输入层**:
  - **输入**：对于每个`time_id`，模型接受一个形状为 `[batch_size, sequence_length, input_size]` 的张量。其中 `sequence_length` 是与特定`time_id`相关的`stock_id`的数量，`input_size` 是股票的特征数量。
  - **输出**：该层的输出直接传递给RNN层。
- **RNN层**:
  - **输入**：来自输入层的数据。
  - **输出**：RNN层输出两部分：输出张量和隐藏状态。输出张量的形状为 `[batch_size, sequence_length, hidden_size]`，而隐藏状态的形状为 `[num_layers, batch_size, hidden_size]`。
- **全连接层**:
  - **输入**：RNN层的输出张量。
  - **输出**：经过全连接层转换后的输出，形状为 `[batch_size, output_size]`。这代表了模型对每个`time_id`的预测。

### **数据加载器**

#### **2. StockDataset**

- `StockDataset` 是一个自定义的PyTorch数据集，它为每个`time_id`生成一个特征张量和一个标签张量。
- 在 `__getitem__` 方法中：
  - 首先，为给定的索引值找到相应的`time_id`。从`df`中筛选出与该`time_id`关联的所有行，并按`stock_id`对它们进行排序。再将这些行中的特征和标签提取为NumPy数组，并转换为PyTorch张量。最后，这些张量被返回，以供数据加载器在训练循环中使用。

### 一些点

- 我这里可调整的参数有学习率和epoch数，在命令行里面分别是`--lr 0.001`和`--epochs 10`这种。学习率我尝试了0.001,0.0001,0.0002，我在写的时候还没训练完，目测一个epoch将近半小时。

- 隐藏层维数暂设128，但是把这个黑盒特征向量给network4会不会太大了？如果可行，需要进一步修改（64,32等）。

- 单纯用这个网络来预测肯定行不大通，个人感觉数据量如此庞大，一般准确预测一支股票的RNN实现都需要上千epochs。我这里区区十来个epochs感觉肯定不够，但也不知道该怎么办了。我睡前打算开几个大的在那边挂着试试看。

- 关于loss，我这里用的就是MSE。

- 我得到的模型文件还没有拿去`test.csv`验证，我也还没写好验证脚本。但是估计验证出来rank_ic会很接近零，毕竟数据量庞大。我打算再检查一遍我的训练脚本之后增加epoch数挂着跑，希望能得到更优质的特征向量。

- 我并没有对数据进行0-1归一化处理。

- **为什么不考虑使用均值**？因为均值都很接近0，很难找到一些与行情有关的特征。

- 总的来说model2寄托了我的全部希望（也许），搞了这么多，限于个人水平和仓促的操作，**也许还会存在根本性的错误**。不过模型脚本完善的还可以，训练起来还算方便，我这边也可以同时换不同参数跑。我这边生成字典已经生成了几个，但是epoch数都很小，所以我打算调试找到相对最好的，希望后面能有一些进展，也希望能积极指正我的错误，也可以提出一些需求（如需要大概多大，范围是多少的向量等）！

现在（8/10 2:49）正在运行的程序：

| learning rate | epochs | hidden size |
| ------------- | ------ | ----------- |
| 0.001         | 1000   | 16          |
| 0.001         | 100    | 32          |
| 0.001         | 100    | 64          |
| 0.001         | 100    | 128         |
| 0.0002        | 1000   | 16          |

我设置了每10个epochs保存一次模型文件，反正模型文件很小，几百kb左右。

（早上起床更新，寄了，感觉30个epoch左右loss就不会怎么变化了，就给停掉了）

### 合理性？

RNN的隐藏状态为数据中的时间模式提供了一个紧凑的表示，能一定程度反映所谓特征，但是缺点明显，一是无法解释，二是向量的维数可能太大。



### Appendix（网上找到的一篇可能对我有用的文章贴在这里）

### 混合神经网络中全连接层的一些技巧

我们做深度学习时往往要结合多种神经网络来构建模型，这种模型构建方式称之为混合神经网络模型，不管是CNN、RNN、BERT、RoBERTa还是混合神经网络模型，最后的最后一定要接一个全连接层来学习提取到的特征并转换为我们输出的维度（此处不讨论实现自注意力机制的全连接和Transformer中的全连接层）。全连接层的设置将最大化的挖掘模型的性能，怎么设置全连接层成了一个重点和难点。
结合自身经验，以下是个人认为的tricks：

1. 批归一化：`nn.BatchNorm1d(dim)`
   批归一化操作是为了将输入中所有特征约束在(0~1)之间，防止强特征数值过大导致忽略弱特征，不过视具体情况而定吧，有的时候输入本身就有强弱之分，这时你归一化反而消除了强特征，多试试。
2. 隐藏层层数：视情况而定，和模型的复杂性成反比。
   我们都知道全连接层一般越多越好，但是必须有非线性激活函数和Dropout，否则再多的线性层也等价于一层，但是实际上层数越多，模型要学习的东西就越复杂，这个时候模型难拟合或者过拟合，尤其是对于复杂模型来说，全连接层往往不需要过深，设置1-2层即可。
3. 神经元个数：第一层适当增加，后续逐层递减。
   根据一些论文，有如下tricks：
   1、隐藏单元的数量不应该超过输入层中单元的两倍
   2、隐藏单元的大小应该介于输入单元和输出单元之间
   3、神经元的数量应捕获输入数据集方差的70~90%
   根据我的实践，第一层适当增加比如100个神经元会取得不错的效果，后续则逐层递减，最后一层就是我们要输出的特征个数。
4. 激活函数：RELU、GELU、ELU。
   传统的激活函数Sigmoid、Tanh把输入控制在（0，1）、（-1，1）之间，容易产生梯度消失和梯度爆炸问题，所以往往要结合RELU：max(0,x)及其变种来防止这个问题，而GELU是预训练模型BERT采用的激活函数，是一种更好更快的激活函数。
   其形式：
   $$\mathrm{GELU}(x)=0.5x(1+\mathrm{tanh} [ 2 / π ( x + 0.044715 x^3 ) ] )$$
